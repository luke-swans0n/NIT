{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAPSTONE TWO: NEGATIVE INCOME TAX EXPERIMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gary_df = pd.read_csv('raw_gary_df.csv', dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gary_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WIDE DATA -> LONG DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_cols = [col for col in gary_df.columns if '-' in col]\n",
    "stubs48_split = []\n",
    "stubs42_split = []\n",
    "stubs9_split = []\n",
    "stubs16_split = []\n",
    "stubs5_split = []\n",
    "id_cols = gary_df.drop(wide_cols, axis =1).columns\n",
    "cols_48 = [col for col in gary_df.columns if '-48' in col]\n",
    "cols_42 = [col for col in gary_df.columns if '-42' in col]\n",
    "cols_9 = [col for col in gary_df.columns if '-9' in col]\n",
    "cols_16 = [col for col in gary_df.columns if '-16' in col]\n",
    "cols_5 = [col for col in gary_df.columns if '-5' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkstring(string, length):\n",
    "    return [string[0+i:length+i] for i in range(0, len(string), length)]\n",
    "for column in wide_cols:\n",
    "    if '-48' in column:\n",
    "        stubs48_split.append(column.split('-')[0])\n",
    "        leng = int(gary_df[column].str.len().unique() /48)\n",
    "        gary_df[column] = gary_df[column].apply(chunkstring, args=[leng])\n",
    "        for i in range(48):\n",
    "            gary_df[column[:-4] + str(i+1)] = gary_df[column].apply(lambda x: x[i])\n",
    "    if '-42' in column:\n",
    "        stubs42_split.append(column.split('-')[0])\n",
    "        leng = int(gary_df[column].str.len().unique() /42)\n",
    "        gary_df[column] = gary_df[column].apply(chunkstring, args=[leng])\n",
    "        for i in range(42):\n",
    "            gary_df[column[:-4] + str(i+1)] = gary_df[column].apply(lambda x: x[i])\n",
    "    if '-9' in column:\n",
    "        stubs9_split.append(column.split('-')[0])\n",
    "        leng = int(gary_df[column].str.len().unique() /9)\n",
    "        gary_df[column] = gary_df[column].apply(chunkstring, args=[leng])\n",
    "        for i in range(9):\n",
    "            gary_df[column[:-3] + str(i+1)] = gary_df[column].apply(lambda x: x[i])  \n",
    "    if '-16' in column:\n",
    "        stubs16_split.append(column.split('-')[0])\n",
    "        leng = int(gary_df[column].str.len().unique() /16)\n",
    "        gary_df[column] = gary_df[column].apply(chunkstring, args=[leng])\n",
    "        for i in range(16):      \n",
    "            gary_df[column[:-3] + str(i+1)] = gary_df[column].apply(lambda x: x[i])\n",
    "    if '-73' in column:\n",
    "        stubs5_split.append(column.split('-')[0])\n",
    "        leng = int(gary_df[column].str.len().unique() /5)\n",
    "        gary_df[column] = gary_df[column].apply(chunkstring, args=[leng])\n",
    "        for i in range(5):\n",
    "            gary_df[column[:-3] + str(i+1)] = gary_df[column].apply(lambda x: x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gary_df.drop(wide_cols,axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stubs48 = [x[:-1] for x in stubs48_split]\n",
    "stubs42 = [x[:-1] for x in stubs42_split]\n",
    "stubs9 = [x[:-1] for x in stubs9_split]\n",
    "stubs16 = [x[:-1] for x in stubs16_split]\n",
    "stubs5 = [x[:-1] for x in stubs5_split]\n",
    "stubs = [stubs48, stubs42, stubs9, stubs16, stubs5]\n",
    "id_list = list(id_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stubs_sorter(df, stubs_list):\n",
    "    cols_list = []\n",
    "    for stub in stubs_list:\n",
    "        lengths = set([len(stub)+1, len(stub)+2])\n",
    "        cols_list += [col for col in df.columns if stub in col and len(col) in lengths]\n",
    "    if any('48' in col for col in cols_list):\n",
    "        special_case = ['EMPSTAT1', 'TYPWRKR7', 'TYPWRKR3', 'TYPWRKR8', 'EMPSTAT7', 'EMPSTAT9',\n",
    "           'TYPWRKR1', 'EMPSTAT8', 'TYPWRKR6', 'TYPWRKR9', 'EMPSTAT2', 'EMPSTAT4',\n",
    "           'TYPWRKR2', 'EMPSTAT3', 'TYPWRKR5', 'EMPSTAT6', 'EMPSTAT5', 'TYPWRKR4']\n",
    "        for col in special_case:\n",
    "            cols_list.remove(col)\n",
    "    #cols_list = [col for col in gary_df.columns if stubs48[0] in col]\n",
    "    cols_list.append('PERNUM')\n",
    "    #gary_df[cols_list]\n",
    "    #gary_df['id'] = gary_df.index\n",
    "    df_long = pd.wide_to_long(gary_df[cols_list], stubnames=stubs_list, i ='PERNUM', j=\"period\")\n",
    "    return df_long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long48 = stubs_sorter(gary_df, stubs48)\n",
    "long42 = stubs_sorter(gary_df, stubs42)\n",
    "long16 = stubs_sorter(gary_df, stubs16)\n",
    "long9 = stubs_sorter(gary_df, stubs9)\n",
    "long5 = stubs_sorter(gary_df, stubs5)\n",
    "longs = [long48, long42, long16, long9, long5]\n",
    "long5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in longs:\n",
    "    df.reset_index(inplace=True)\n",
    "    df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map9 = {1 : 1, 2 : 14, 3 : 18, 4 : 22, 5 : 26, 6 : 31, 7 : 35, 8 : 38, 9 : 43}\n",
    "map5 = {91 : 1, 92 : 13, 93 : 25, 94 : 37, 95 : 48}\n",
    "long16['period'] = long16['period']*4\n",
    "long9['period'] = long9['period'].map(map9)\n",
    "long5['period'] = long5['period'].map(map5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long9.sort_values(by = ['PERNUM', 'period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long5.sort_values(by = ['PERNUM', 'period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gary_final = pd.merge(long5, long9, on=['PERNUM', 'period'])\n",
    "gary_final.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gary_final.loc[gary_final['PERNUM'] =='500001']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HANDLING NULLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropping families with no people and people with no families (this was due to a record-keeping error on the part of the experimenters. Families starting with number 4 are supposed to be in the Sacramento file, not Gary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting some of the more-common missing data codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gary_df.replace(['9997', '9999','9993','9994', '97', '93'], np.NaN, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropping the columns with > 75% of their entries being left blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_miss(df):\n",
    "    # returns the percent of entries that are None in each column.\n",
    "   return df.isnull().sum()/df.isnull().count()\n",
    "bad_cols = gary_df.loc[:,(percent_miss(gary_df) > 0.75)].columns\n",
    "gary_df.drop(bad_cols, axis=1, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATTDATE stands for attrition date, meaning what date the family left the experiment before it ended. These families left because they either moved away, stopped responding to experimenters, or the active filing member passed. They're being dropped here as we are interested in effects of welfare over time and these cutoff early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gary_df = gary_df.loc[gary_df['ATTDATE'] == '00000',:]\n",
    "gary_df.drop(['ATTDATE', 'FAMNUM'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the comments within this section are ideas for further analysis, or methods of data cleaning attempted that either failed or were too large of a time sink to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gary_df = gary_df.loc[gary_df['TREATLEV'] != '0']\n",
    "#gary_control_df = gary_df.loc[gary_df['TREATLEV'] == '0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gary_df.set_index('PERNUM', inplace=True)\n",
    "gary_simp_df = pd.get_dummies(gary_df['TREATLEV'], drop_first=True)\n",
    "#gary_df.drop(['TREATLEV'], axis=1, inplace=True)\n",
    "col_dict = {1:'TREATLEV_1', 2: 'TREATLEV_2', 3:'TREATLEV_3', 4:'TREATLEV_4'}\n",
    "gary_simp_df.rename(columns = col_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gary_simp_df = pd.concat([gary_simp_df, pd.get_dummies(gary_df['POVLEV'], drop_first=True)], axis=1)\n",
    "#gary_df.drop(['POVLEV'], axis=1, inplace=True)\n",
    "col_dict = {2:'POV_LEV_2', 3: 'POV_LEV_3', 4:'POV_LEV_4', 5:'POV_LEV_5'}\n",
    "gary_simp_df.rename(columns = col_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of comments was my attempt to take the time-data and parse it out to hopefully generate new rows of data from them. Given more time, I would greatly expand this section, as it has the most potential and would give me the tools to make good features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def chunkstring(string, length):\n",
    "    return [string[0+i:length+i] for i in range(0, len(string), length)]\n",
    "for column in gary_df.columns:\n",
    "    if '-48' in column:\n",
    "        leng = int(gary_df[column].str.len().unique() /48)\n",
    "        gary_df[column] = gary_df[column].apply(chunkstring, args=[leng])\n",
    "    if '-43' in column:\n",
    "        leng = int(gary_df[column].str.len().unique() /43)\n",
    "        gary_df[column] = gary_df[column].apply(chunkstring, args=[leng])\n",
    "    if '-42' in column:\n",
    "        leng = int(gary_df[column].str.len().unique() /42)\n",
    "        gary_df[column] = gary_df[column].apply(chunkstring, args=[leng])\n",
    "    if '-9' in column:\n",
    "        leng = int(gary_df[column].str.len().unique() /9)\n",
    "        gary_df[column] = gary_df[column].apply(chunkstring, args=[leng])\n",
    " '''       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for column in gary_df.columns:\n",
    "    if '-48' in column:\n",
    "        leng = int(gary_df[column].str.len().unique() /48)\n",
    "        basename = column\n",
    "        for i in range(48):\n",
    "            gary_df[basename+'-Month'+str(i)] = gary_df[column][i:i+leng]\n",
    "gary_df.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming the column for the sake of ease in coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gary_df.set_index('PERNUM', inplace=True)\n",
    "#periodic_columns =['SSI1-48', 'TTI1-48', 'SS1-48', 'VA1-48', 'MISINC1-48',\n",
    "#                   'OTHINC1-48', 'JOBINC1-48', 'DAYINC1-48', 'OJINC1-48',\n",
    "#                  'UEMBEN1-48', 'STRKWC1-48']\n",
    "gary_df.rename(columns= {'EMPSTAT1-9': 'EMPSTAT'}, inplace = True)\n",
    "gary_df.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ruling out persons for which employment status data was never collected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gary_simp_df['EMPSTAT'] = gary_df.loc[gary_df.EMPSTAT.str.contains('00|01|02', regex=True),'EMPSTAT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gary_simp_df.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unemployed + Actively seeking work, Employed -> in the labor force -> 1   \n",
    "Unemployed + not actively seeking work -> not in labor force -> 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gary_simp_df.loc[(gary_simp_df.EMPSTAT.str.contains('(01)', regex=True)),'EMPSTAT']= '1'\n",
    "gary_simp_df.loc[(gary_simp_df.EMPSTAT.str.contains('(00)', regex=True)),'EMPSTAT']= '1'\n",
    "#gary_simp_df.loc[(gary_simp_df.EMPSTAT.str.contains('(00)(02)', regex=True)),'EMPSTAT']= 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gary_simp_df.loc[gary_simp_df['EMPSTAT'] != '1', 'EMPSTAT'] ='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gary_simp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= gary_simp_df.drop('EMPSTAT', axis=1)\n",
    "Y= gary_simp_df['EMPSTAT']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state=2, stratify = Y)\n",
    "print(y_train, y_test, X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier(strategy = 'most_frequent')\n",
    "dummy.fit(X_train, y_train)\n",
    "dummy.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_report = classification_report(y_test, dummy.predict(X_test), target_names = ['Not in Labor', 'In Labor']\n",
    "print(dummy_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "grid = GridSearchCV(estimator=lr,\\\n",
    "                   param_grid = { \\\n",
    "                                'C' : np.arange(0.05, 1.0, .05),\\\n",
    "                                'penalty' : ['l2'],\\\n",
    "                                'max_iter' : np.arange(500, 5000, 500)},\n",
    "                   verbose = 2)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, grid.predict(X_test), target_names=['Not in Labor', 'In Labor']))\n",
    "print(confusion_matrix(y_test, grid.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE:** Logistic Regression does *no* better than guessing the most frequent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = ensemble.RandomForestClassifier()\n",
    "grid = GridSearchCV(estimator = rfc,\n",
    "                         param_grid={\\\n",
    "                                    'max_depth' : [1, 2, 3],\\\n",
    "                                    'criterion':['gini', 'entropy'],\\\n",
    "                                    'min_samples_split' : np.arange(0.05,1.0, 0.05),\\\n",
    "                                             },\n",
    "                   verbose=2)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, grid.predict(X_test), target_names=['Not in Labor', 'In Labor']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE** Random Forest doest *no* better than guessing the most frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "rand = RandomizedSearchCV(estimator=clf,\\\n",
    "                   param_distributions = { \\\n",
    "                                'n_estimators' : np.arange(500, 1000, 250),\\\n",
    "                                'max_depth' : np.arange(1,3),\\\n",
    "                                'learning_rate' : np.arange(0.1, .90, 0.1)})\n",
    "rand.fit(X_train, y_train)\n",
    "print(classification_report(y_test, rand.predict(X_test), target_names=['Not in Labor', 'In Labor']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting does tremendously better at predicting not in the labor force, and thus is the best model of the bunch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FURTHER CONSIDERATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the comments throughout this notebook better analysis would come from:  \n",
    "1. Parsing out the 1-48, 1-43, 1-42, 1-9 columns and making them into rows by adding a column for month. The index could then be Person, Month for the data frame. \n",
    "2. Using ffill to patch up a lot of the NA's that are either dropped or ignored in this notebook.\n",
    "3. Running the models with all the features (but not using GridSearch/RandomizedSearch), then doing some basic feature reduction (PCA, etc.)\n",
    "4. Reconsidering the structure of the categorical data TREATLEV, POVLEV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gary_df['NOTINFR'] = gary_df['EMPINT'].str.contains('02')\n",
    "#gary_df['NOTINFR'] = gary_df['NOTINFR'].astype(int)\n",
    "#gary_df['EMPGAIN'] = gary_df['EMPINT'].str.contains('(00)(01)', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gary_df['EMPLOSS'] = gary_df['EMPINT'].str.contains('(01)(00)', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gary_df['EMPGAIN'] = gary_df.EMPGAIN.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gary_df['EMPLOSS'] = gary_df.EMPLOSS.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns_for_later = ['EMPLOSS', 'EMPGAIN']\n",
    "#gary_df.drop(columns_for_later, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gary_df['EMP'] = gary_df['EMPINT'].str.contains('01')\n",
    "#gary_df['EMP'] = gary_df.EMP.astype(int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
